{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc30107d",
   "metadata": {},
   "source": [
    "## Project\n",
    "\n",
    "Name : Ramya Gopinath\n",
    "\n",
    "USC ID : 4595082262\n",
    "\n",
    "Github username : ramyagopinath08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "0d37d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements for tensorflow and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "#import statements for pabdas and numpy\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#histogram \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import statements for all the models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "0995598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "900db20a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "dddb83b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install tfds-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "0c46524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "#checking the version of the installed tensorflow\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "bb83d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78fce38",
   "metadata": {},
   "source": [
    "## 1. Text Classification\n",
    "\n",
    "**It is highly recommended that you complete this project using Keras1 and Python.**\n",
    "\n",
    "**(a) In this problem, we are trying to build a classifier to analyze the sentiment of\n",
    "reviews. You are provided with text data in two folders: one folder involves\n",
    "positive reviews, and one folder involves negative reviews.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "c6314047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1400 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Reading the data, the data is divided into test and train\n",
    "#Each of the train and test folders have pos and neg folders which have the text review files\n",
    "#Once the data is read, the data is stored into a dataframe\n",
    "\n",
    "#Reading the train data\n",
    "dataset_train = keras.preprocessing.text_dataset_from_directory('../Data/train/', batch_size=None)\n",
    "df_train = tfds.as_dataframe(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "e3d91c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Reading the test data\n",
    "dataset_test = keras.preprocessing.text_dataset_from_directory('../Data/test/', batch_size=None)\n",
    "df_test = tfds.as_dataframe(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c2d5c",
   "metadata": {},
   "source": [
    "## (b) Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2e31b4",
   "metadata": {},
   "source": [
    "**i. You can use binary encoding for the sentiments , i.e y = 1 for positive senti￾ments and y = −1 for negative sentiments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "04ade825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'at times , you\\'d think edtv would be an ent...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'oliver stone\\'s latest feature is the last o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'it\\'s a rare treat when a quality horror fil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'i wish i could accurately describe the theme...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'studio expectations must not have been high ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "0  b'at times , you\\'d think edtv would be an ent...       0\n",
       "1  b'oliver stone\\'s latest feature is the last o...       1\n",
       "2  b'it\\'s a rare treat when a quality horror fil...       0\n",
       "3  b'i wish i could accurately describe the theme...       0\n",
       "4  b'studio expectations must not have been high ...       1"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assigning names to the columns\n",
    "#Labels have pos=1 and neg=0 classes\n",
    "df_train.columns = ['Text','Labels']\n",
    "df_train['Text'] = df_train['Text'].astype(str)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "6e31905b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'in the year 2029 , captain leo davidson ( ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'the following review encompasses two version...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'what do you get when you combine clueless an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b\"anastasia contains something that has been l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'okay , bear with me y\\'all , cause first off...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "0  b'in the year 2029 , captain leo davidson ( ma...       0\n",
       "1  b'the following review encompasses two version...       0\n",
       "2  b'what do you get when you combine clueless an...       1\n",
       "3  b\"anastasia contains something that has been l...       1\n",
       "4  b'okay , bear with me y\\'all , cause first off...       0"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assigning names to the columns\n",
    "#Labels have pos=1 and neg=0 classes\n",
    "df_test.columns = ['Text','Labels']\n",
    "df_test['Text'] = df_test['Text'].astype(str)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "63d1367a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text      object\n",
       "Labels     int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bca680",
   "metadata": {},
   "source": [
    "**ii. The data are pretty clean. Remove the punctuation and numbers from the\n",
    "data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "51941ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a self-defined function to remove the numbers, special characters from each of the file that is read\n",
    "def data_cleaning(data):\n",
    "    data['Text'] = data['Text'].str.replace(r'[\\\\][n]','')\n",
    "    data['Text'] = data['Text'].str.replace('[0-9]+','')\n",
    "    data['Text'] = data['Text'].str.replace(r'[^\\w\\s]+','')\n",
    "    data['Text'] = data['Text'].str.replace('b','',n=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "de0dffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the text/review data in both train and text dataframes\n",
    "df_train = data_cleaning(df_train)\n",
    "df_test = data_cleaning(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b3d770",
   "metadata": {},
   "source": [
    "**iii. The name of each text file starts with cv number. Use text files 0-699 in each\n",
    "class for training and 700-999 for testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "ceb63fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aggressive  bleak  and unrelenting film about ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>synopsis  upper middle class  suburban family ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>practical magic   is a film that is so misgu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>footloose  has only one goal in mind  to ree...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis  a humorless police officers life cha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "0  aggressive  bleak  and unrelenting film about ...       1\n",
       "1  synopsis  upper middle class  suburban family ...       1\n",
       "2    practical magic   is a film that is so misgu...       0\n",
       "3    footloose  has only one goal in mind  to ree...       1\n",
       "4  synopsis  a humorless police officers life cha...       0"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "d1d26c5d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in the year   captain leo davidson  mark wahlb...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the following review encompasses two versions ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what do you get when you combine clueless and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anastasia contains something that has been lac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>okay  bear with me yall  cause first off i hav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Labels\n",
       "0  in the year   captain leo davidson  mark wahlb...       0\n",
       "1  the following review encompasses two versions ...       0\n",
       "2  what do you get when you combine clueless and ...       1\n",
       "3  anastasia contains something that has been lac...       1\n",
       "4  okay  bear with me yall  cause first off i hav...       0"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20268a75",
   "metadata": {},
   "source": [
    "**iv. Count the number of unique words in the whole dataset (train + test) and\n",
    "print it out.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "9cf3fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining train and test data\n",
    "combined_dataset = pd.concat([df_train,df_test], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "6bbda3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46777\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the data to find the unique words in the data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(combined_dataset['Text'])\n",
    "unique_words = tokenizer.word_counts\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "de8f948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('aggressive', 21),\n",
       "             ('bleak', 50),\n",
       "             ('and', 35352),\n",
       "             ('unrelenting', 5),\n",
       "             ('film', 8861),\n",
       "             ('about', 3519),\n",
       "             ('an', 5736),\n",
       "             ('interracial', 5),\n",
       "             ('couple', 420),\n",
       "             ('steve', 151),\n",
       "             ('sam', 163),\n",
       "             ('damon', 69),\n",
       "             ('jones', 170),\n",
       "             ('elexa', 1),\n",
       "             ('williams', 217),\n",
       "             ('respectively', 39),\n",
       "             ('who', 5378),\n",
       "             ('are', 6945),\n",
       "             ('viciously', 7),\n",
       "             ('attacked', 28),\n",
       "             ('in', 21600),\n",
       "             ('a', 37966),\n",
       "             ('parking', 15),\n",
       "             ('garage', 14),\n",
       "             ('one', 5522),\n",
       "             ('night', 413),\n",
       "             ('by', 6219),\n",
       "             ('gang', 100),\n",
       "             ('of', 33974),\n",
       "             ('skinheads', 10),\n",
       "             ('the', 76336),\n",
       "             ('beat', 62),\n",
       "             ('then', 1407),\n",
       "             ('force', 161),\n",
       "             ('him', 2631),\n",
       "             ('to', 31764),\n",
       "             ('watch', 601),\n",
       "             ('as', 11343),\n",
       "             ('they', 4278),\n",
       "             ('brutally', 33),\n",
       "             ('rape', 44),\n",
       "             ('his', 9569),\n",
       "             ('girlfriend', 195),\n",
       "             ('over', 1301),\n",
       "             ('again', 681),\n",
       "             ('when', 3254),\n",
       "             ('kills', 77),\n",
       "             ('herself', 176),\n",
       "             ('later', 381),\n",
       "             ('that', 15112),\n",
       "             ('evening', 39),\n",
       "             ('decides', 188),\n",
       "             ('must', 606),\n",
       "             ('be', 6082),\n",
       "             ('taught', 19),\n",
       "             ('lesson', 37),\n",
       "             ('waiting', 113),\n",
       "             ('seven', 135),\n",
       "             ('months', 112),\n",
       "             ('for', 9914),\n",
       "             ('thuggish', 4),\n",
       "             ('forget', 122),\n",
       "             ('incident', 37),\n",
       "             ('entirely', 142),\n",
       "             ('shaves', 2),\n",
       "             ('head', 350),\n",
       "             ('dresses', 14),\n",
       "             ('combat', 35),\n",
       "             ('boots', 8),\n",
       "             ('suspenders', 2),\n",
       "             ('tattoos', 7),\n",
       "             ('flesh', 51),\n",
       "             ('with', 10779),\n",
       "             ('nazi', 29),\n",
       "             ('symbols', 12),\n",
       "             ('tries', 369),\n",
       "             ('infiltrate', 4),\n",
       "             ('he', 7611),\n",
       "             ('gets', 864),\n",
       "             ('closer', 53),\n",
       "             ('people', 1420),\n",
       "             ('committed', 40),\n",
       "             ('horrible', 102),\n",
       "             ('crime', 199),\n",
       "             ('against', 366),\n",
       "             ('begins', 392),\n",
       "             ('learn', 172),\n",
       "             ('just', 2900),\n",
       "             ('how', 1509),\n",
       "             ('vicious', 41),\n",
       "             ('these', 1307),\n",
       "             ('really', 1563),\n",
       "             ('starts', 316),\n",
       "             ('question', 277),\n",
       "             ('own', 813),\n",
       "             ('motives', 34),\n",
       "             ('violence', 298),\n",
       "             ('randolph', 1),\n",
       "             ('krets', 2),\n",
       "             ('script', 762),\n",
       "             ('based', 370),\n",
       "             ('on', 7251),\n",
       "             ('two', 1825),\n",
       "             ('real', 858),\n",
       "             ('life', 1472),\n",
       "             ('incidents', 12),\n",
       "             ('affected', 24),\n",
       "             ('not', 5528),\n",
       "             ('only', 2483),\n",
       "             ('but', 8584),\n",
       "             ('films', 2103),\n",
       "             ('producer', 99),\n",
       "             ('shaun', 2),\n",
       "             ('hill', 94),\n",
       "             ('is', 25179),\n",
       "             ('affront', 2),\n",
       "             ('intolerance', 5),\n",
       "             ('causes', 64),\n",
       "             ('its', 5987),\n",
       "             ('audience', 878),\n",
       "             ('subjected', 25),\n",
       "             ('such', 1217),\n",
       "             ('brutality', 17),\n",
       "             ('ignorance', 10),\n",
       "             ('cant', 645),\n",
       "             ('help', 532),\n",
       "             ('disgusted', 11),\n",
       "             ('events', 201),\n",
       "             ('presented', 104),\n",
       "             ('within', 220),\n",
       "             ('ive', 382),\n",
       "             ('seen', 918),\n",
       "             ('it', 12302),\n",
       "             ('said', 353),\n",
       "             ('this', 9565),\n",
       "             ('fight', 315),\n",
       "             ('club', 124),\n",
       "             ('should', 864),\n",
       "             ('have', 4899),\n",
       "             ('been', 2045),\n",
       "             ('david', 365),\n",
       "             ('finchers', 13),\n",
       "             ('was', 4934),\n",
       "             ('trying', 565),\n",
       "             ('define', 13),\n",
       "             ('generation', 93),\n",
       "             ('wake', 30),\n",
       "             ('up', 3108),\n",
       "             ('call', 211),\n",
       "             ('message', 177),\n",
       "             ('powerful', 176),\n",
       "             ('if', 2791),\n",
       "             ('fincher', 34),\n",
       "             ('kubrick', 29),\n",
       "             ('our', 600),\n",
       "             ('kret', 1),\n",
       "             ('d', 131),\n",
       "             ('w', 18),\n",
       "             ('griffith', 24),\n",
       "             ('thematically', 10),\n",
       "             ('speaking', 89),\n",
       "             ('there', 2758),\n",
       "             ('single', 218),\n",
       "             ('likable', 76),\n",
       "             ('character', 1906),\n",
       "             ('obviously', 226),\n",
       "             ('most', 2300),\n",
       "             ('repulsive', 9),\n",
       "             ('characters', 1947),\n",
       "             ('because', 1682),\n",
       "             ('their', 3116),\n",
       "             ('beliefs', 22),\n",
       "             ('constant', 77),\n",
       "             ('state', 144),\n",
       "             ('fury', 20),\n",
       "             ('other', 1925),\n",
       "             ('races', 19),\n",
       "             ('circles', 21),\n",
       "             ('africanamerican', 21),\n",
       "             ('constantly', 142),\n",
       "             ('shown', 129),\n",
       "             ('sexually', 25),\n",
       "             ('assaulting', 5),\n",
       "             ('drug', 147),\n",
       "             ('abusers', 2),\n",
       "             ('always', 574),\n",
       "             ('joyride', 4),\n",
       "             ('looking', 436),\n",
       "             ('someone', 375),\n",
       "             ('accost', 1),\n",
       "             ('even', 2555),\n",
       "             ('gay', 105),\n",
       "             ('men', 516),\n",
       "             ('get', 1921),\n",
       "             ('act', 277),\n",
       "             ('locating', 7),\n",
       "             ('lair', 10),\n",
       "             ('beating', 46),\n",
       "             ('them', 1877),\n",
       "             ('severely', 14),\n",
       "             ('lead', 248),\n",
       "             ('pipes', 5),\n",
       "             ('blunt', 10),\n",
       "             ('objects', 29),\n",
       "             ('main', 399),\n",
       "             ('also', 1965),\n",
       "             ('unlikable', 16),\n",
       "             ('person', 326),\n",
       "             ('becomes', 526),\n",
       "             ('seek', 31),\n",
       "             ('out', 3441),\n",
       "             ('revenge', 90),\n",
       "             ('what', 3079),\n",
       "             ('all', 4251),\n",
       "             ('comes', 733),\n",
       "             ('down', 874),\n",
       "             ('simple', 281),\n",
       "             ('begets', 2),\n",
       "             ('clashes', 6),\n",
       "             ('between', 954),\n",
       "             ('factions', 5),\n",
       "             ('will', 2195),\n",
       "             ('do', 1893),\n",
       "             ('nothing', 800),\n",
       "             ('continue', 70),\n",
       "             ('endless', 55),\n",
       "             ('cycle', 10),\n",
       "             ('inherent', 19),\n",
       "             ('society', 149),\n",
       "             ('cast', 760),\n",
       "             ('absolutely', 186),\n",
       "             ('remarkable', 72),\n",
       "             ('shows', 432),\n",
       "             ('raw', 32),\n",
       "             ('energy', 121),\n",
       "             ('many', 1268),\n",
       "             ('actors', 716),\n",
       "             ('seem', 574),\n",
       "             ('summon', 2),\n",
       "             ('days', 344),\n",
       "             ('performance', 885),\n",
       "             ('seems', 1032),\n",
       "             ('less', 452),\n",
       "             ('than', 2438),\n",
       "             ('initial', 53),\n",
       "             ('reaction', 58),\n",
       "             ('hearing', 50),\n",
       "             ('girlfriends', 23),\n",
       "             ('suicide', 60),\n",
       "             ('affecting', 19),\n",
       "             ('scenes', 1265),\n",
       "             ('i', 7507),\n",
       "             ('ever', 737),\n",
       "             ('lee', 245),\n",
       "             ('wilson', 58),\n",
       "             ('plays', 753),\n",
       "             ('appropriately', 41),\n",
       "             ('enough', 907),\n",
       "             ('accurately', 18),\n",
       "             ('evil', 413),\n",
       "             ('second', 366),\n",
       "             ('command', 41),\n",
       "             ('skinhead', 14),\n",
       "             ('makes', 992),\n",
       "             ('extra', 60),\n",
       "             ('money', 501),\n",
       "             ('performing', 27),\n",
       "             ('oral', 10),\n",
       "             ('sex', 447),\n",
       "             ('irony', 38),\n",
       "             ('lost', 403),\n",
       "             ('wilsons', 2),\n",
       "             ('pure', 90),\n",
       "             ('hatred', 21),\n",
       "             ('brings', 202),\n",
       "             ('hate', 130),\n",
       "             ('perfectly', 164),\n",
       "             ('sadly', 103),\n",
       "             ('actor', 513),\n",
       "             ('dave', 29),\n",
       "             ('ward', 25),\n",
       "             ('portrays', 42),\n",
       "             ('crew', 207),\n",
       "             ('able', 339),\n",
       "             ('follow', 164),\n",
       "             ('excellent', 184),\n",
       "             ('victim', 100),\n",
       "             ('himself', 656),\n",
       "             ('at', 4967),\n",
       "             ('hands', 142),\n",
       "             ('fellow', 107),\n",
       "             ('during', 654),\n",
       "             ('road', 121),\n",
       "             ('rage', 35),\n",
       "             ('showed', 57),\n",
       "             ('immense', 14),\n",
       "             ('promise', 78),\n",
       "             ('greatly', 37),\n",
       "             ('missed', 70),\n",
       "             ('note', 233),\n",
       "             ('shame', 94),\n",
       "             ('filmmakers', 208),\n",
       "             ('had', 1544),\n",
       "             ('add', 176),\n",
       "             ('disclaimer', 6),\n",
       "             ('end', 1047),\n",
       "             ('stating', 9),\n",
       "             ('neither', 158),\n",
       "             ('nor', 182),\n",
       "             ('musicians', 20),\n",
       "             ('contributing', 5),\n",
       "             ('music', 475),\n",
       "             ('condone', 6),\n",
       "             ('racism', 31),\n",
       "             ('couldnt', 185),\n",
       "             ('spelled', 7),\n",
       "             ('any', 1573),\n",
       "             ('more', 3345),\n",
       "             ('clearly', 132),\n",
       "             ('here', 1108),\n",
       "             ('those', 1013),\n",
       "             ('would', 2042),\n",
       "             ('probably', 538),\n",
       "             ('find', 779),\n",
       "             ('much', 2024),\n",
       "             ('akin', 13),\n",
       "             ('training', 43),\n",
       "             ('video', 296),\n",
       "             ('condemnation', 8),\n",
       "             ('no', 2413),\n",
       "             ('winners', 12),\n",
       "             ('either', 385),\n",
       "             ('or', 3097),\n",
       "             ('her', 4504),\n",
       "             ('comeuppance', 9),\n",
       "             ('ending', 421),\n",
       "             ('leaves', 181),\n",
       "             ('us', 1071),\n",
       "             ('almost', 811),\n",
       "             ('right', 782),\n",
       "             ('where', 1459),\n",
       "             ('we', 2360),\n",
       "             ('began', 75),\n",
       "             ('american', 517),\n",
       "             ('history', 215),\n",
       "             ('x', 56),\n",
       "             ('redemption', 33),\n",
       "             ('victories', 7),\n",
       "             ('store', 118),\n",
       "             ('pariahs', 1),\n",
       "             ('band', 119),\n",
       "             ('twisted', 53),\n",
       "             ('souls', 26),\n",
       "             ('carries', 49),\n",
       "             ('despite', 351),\n",
       "             ('you', 4438),\n",
       "             ('may', 854),\n",
       "             ('momentarily', 7),\n",
       "             ('put', 420),\n",
       "             ('your', 843),\n",
       "             ('mind', 416),\n",
       "             ('rest', 431),\n",
       "             ('still', 1038),\n",
       "             ('forms', 38),\n",
       "             ('very', 1864),\n",
       "             ('little', 1491),\n",
       "             ('can', 2232),\n",
       "             ('done', 458),\n",
       "             ('eradicate', 1),\n",
       "             ('synopsis', 67),\n",
       "             ('upper', 31),\n",
       "             ('middle', 197),\n",
       "             ('class', 132),\n",
       "             ('suburban', 16),\n",
       "             ('family', 620),\n",
       "             ('man', 1271),\n",
       "             ('lester', 20),\n",
       "             ('kevin', 305),\n",
       "             ('spacey', 45),\n",
       "             ('realizes', 72),\n",
       "             ('going', 871),\n",
       "             ('through', 1180),\n",
       "             ('motions', 29),\n",
       "             ('unable', 55),\n",
       "             ('feel', 461),\n",
       "             ('passion', 58),\n",
       "             ('soulless', 11),\n",
       "             ('cynical', 36),\n",
       "             ('longer', 129),\n",
       "             ('edge', 95),\n",
       "             ('success', 216),\n",
       "             ('failure', 74),\n",
       "             ('everyone', 445),\n",
       "             ('else', 371),\n",
       "             ('around', 896),\n",
       "             ('has', 4715),\n",
       "             ('similar', 186),\n",
       "             ('symptoms', 4),\n",
       "             ('lesters', 2),\n",
       "             ('unassertive', 1),\n",
       "             ('daughter', 296),\n",
       "             ('jane', 80),\n",
       "             ('thora', 8),\n",
       "             ('birch', 27),\n",
       "             ('too', 1539),\n",
       "             ('lethargic', 3),\n",
       "             ('change', 204),\n",
       "             ('world', 963),\n",
       "             ('yet', 600),\n",
       "             ('ready', 117),\n",
       "             ('whine', 6),\n",
       "             ('complain', 20),\n",
       "             ('wife', 548),\n",
       "             ('carolyn', 6),\n",
       "             ('annette', 26),\n",
       "             ('bening', 26),\n",
       "             ('reduced', 30),\n",
       "             ('keeping', 87),\n",
       "             ('appearances', 48),\n",
       "             ('reciting', 11),\n",
       "             ('commercialist', 1),\n",
       "             ('slogans', 7),\n",
       "             ('big', 971),\n",
       "             ('house', 367),\n",
       "             ('decent', 164),\n",
       "             ('standard', 104),\n",
       "             ('living', 229),\n",
       "             ('appreciate', 91),\n",
       "             ('beauty', 136),\n",
       "             ('freedom', 64),\n",
       "             ('deprived', 8),\n",
       "             ('dopedealing', 1),\n",
       "             ('teen', 126),\n",
       "             ('from', 4986),\n",
       "             ('across', 219),\n",
       "             ('street', 130),\n",
       "             ('practically', 53),\n",
       "             ('imprisoned', 21),\n",
       "             ('domineering', 6),\n",
       "             ('bullying', 7),\n",
       "             ('father', 422),\n",
       "             ('embarks', 6),\n",
       "             ('mad', 82),\n",
       "             ('scramble', 1),\n",
       "             ('fact', 795),\n",
       "             ('quit', 21),\n",
       "             ('job', 550),\n",
       "             ('like', 3555),\n",
       "             ('newfound', 13),\n",
       "             ('interest', 226),\n",
       "             ('exercise', 53),\n",
       "             ('impress', 30),\n",
       "             ('teenage', 103),\n",
       "             ('girl', 404),\n",
       "             ('ruffle', 1),\n",
       "             ('lot', 675),\n",
       "             ('feathers', 2),\n",
       "             ('regain', 9),\n",
       "             ('ability', 140),\n",
       "             ('opinion', 100),\n",
       "             ('say', 802),\n",
       "             ('time', 2283),\n",
       "             ('never', 1364),\n",
       "             ('good', 2321),\n",
       "             ('health', 19),\n",
       "             ('until', 430),\n",
       "             ('gone', 184),\n",
       "             ('likewise', 15),\n",
       "             ('raised', 46),\n",
       "             ('fabulously', 5),\n",
       "             ('pampered', 4),\n",
       "             ('prosperity', 2),\n",
       "             ('become', 509),\n",
       "             ('unhappy', 31),\n",
       "             ('spiteful', 8),\n",
       "             ('dumbest', 9),\n",
       "             ('inconsequential', 8),\n",
       "             ('problems', 293),\n",
       "             ('simply', 428),\n",
       "             ('clue', 45),\n",
       "             ('sometimes', 269),\n",
       "             ('takes', 668),\n",
       "             ('near', 168),\n",
       "             ('fall', 232),\n",
       "             ('accidentally', 57),\n",
       "             ('deliberate', 12),\n",
       "             ('fasting', 1),\n",
       "             ('move', 171),\n",
       "             ('apathy', 2),\n",
       "             ('reclaim', 9),\n",
       "             ('challenge', 51),\n",
       "             ('deprivationappreciation', 1),\n",
       "             ('major', 294),\n",
       "             ('theme', 129),\n",
       "             ('nonformula', 2),\n",
       "             ('intelligent', 181),\n",
       "             ('articulate', 13),\n",
       "             ('movie', 5441),\n",
       "             ('offers', 147),\n",
       "             ('fantastic', 78),\n",
       "             ('performances', 430),\n",
       "             ('especially', 454),\n",
       "             ('gosh', 8),\n",
       "             ('darn', 24),\n",
       "             ('amazing', 181),\n",
       "             ('thing', 804),\n",
       "             ('sentimental', 39),\n",
       "             ('hospital', 77),\n",
       "             ('scene', 1373),\n",
       "             ('overly', 65),\n",
       "             ('dramatic', 189),\n",
       "             ('hokum', 9),\n",
       "             ('entertains', 4),\n",
       "             ('brilliant', 176),\n",
       "             ('mood', 94),\n",
       "             ('thoughtful', 30),\n",
       "             ('flick', 192),\n",
       "             ('highly', 129),\n",
       "             ('recommend', 106),\n",
       "             ('practical', 19),\n",
       "             ('magic', 92),\n",
       "             ('so', 3584),\n",
       "             ('misguided', 30),\n",
       "             ('ways', 187),\n",
       "             ('wonder', 246),\n",
       "             ('talented', 150),\n",
       "             ('highprofile', 16),\n",
       "             ('involved', 241),\n",
       "             ('embarrassing', 54),\n",
       "             ('claptrap', 2),\n",
       "             ('stars', 383),\n",
       "             ('sandra', 48),\n",
       "             ('bullock', 31),\n",
       "             ('nicole', 29),\n",
       "             ('kidman', 28),\n",
       "             ('sally', 25),\n",
       "             ('gillian', 14),\n",
       "             ('owens', 20),\n",
       "             ('sisters', 49),\n",
       "             ('whose', 434),\n",
       "             ('long', 751),\n",
       "             ('line', 401),\n",
       "             ('witches', 19),\n",
       "             ('spanning', 1),\n",
       "             ('back', 1034),\n",
       "             ('years', 970),\n",
       "             ('start', 306),\n",
       "             ('parents', 223),\n",
       "             ('die', 188),\n",
       "             ('children', 282),\n",
       "             ('go', 1075),\n",
       "             ('live', 325),\n",
       "             ('zany', 26),\n",
       "             ('aunts', 3),\n",
       "             ('stockard', 6),\n",
       "             ('channing', 8),\n",
       "             ('dianne', 4),\n",
       "             ('wiest', 4),\n",
       "             ('switch', 30),\n",
       "             ('present', 179),\n",
       "             ('day', 615),\n",
       "             ('stronger', 44),\n",
       "             ('rebellious', 12),\n",
       "             ('sibling', 9),\n",
       "             ('home', 546),\n",
       "             ('meets', 210),\n",
       "             ('dark', 309),\n",
       "             ('abusive', 19),\n",
       "             ('guy', 620),\n",
       "             ('goran', 4),\n",
       "             ('visjnic', 2),\n",
       "             ('while', 1537),\n",
       "             ('stays', 39),\n",
       "             ('hometown', 18),\n",
       "             ('falls', 239),\n",
       "             ('love', 1089),\n",
       "             ('sweet', 160),\n",
       "             ('caring', 41),\n",
       "             ('devastated', 8),\n",
       "             ('husband', 224),\n",
       "             ('hit', 280),\n",
       "             ('truck', 48),\n",
       "             ('killed', 215),\n",
       "             ('come', 772),\n",
       "             ('think', 842),\n",
       "             ('shes', 407),\n",
       "             ('since', 768),\n",
       "             ('she', 2687),\n",
       "             ('minutes', 643),\n",
       "             ('calls', 100),\n",
       "             ('after', 1754),\n",
       "             ('having', 548),\n",
       "             ('another', 1111),\n",
       "             ('spat', 2),\n",
       "             ('murder', 246),\n",
       "             ('desperation', 23),\n",
       "             ('bury', 14),\n",
       "             ('body', 258),\n",
       "             ('backyard', 6),\n",
       "             ('wildly', 42),\n",
       "             ('convoluted', 31),\n",
       "             ('story', 2121),\n",
       "             ('subplots', 66),\n",
       "             ('involving', 242),\n",
       "             ('dead', 365),\n",
       "             ('rising', 85),\n",
       "             ('exorcism', 2),\n",
       "             ('mention', 113),\n",
       "             ('spattering', 2),\n",
       "             ('lighthearted', 38),\n",
       "             ('whimsy', 4),\n",
       "             ('pretty', 523),\n",
       "             ('idea', 378),\n",
       "             ('messy', 11),\n",
       "             ('seeing', 336),\n",
       "             ('reflected', 6),\n",
       "             ('element', 149),\n",
       "             ('liked', 154),\n",
       "             ('enjoyed', 106),\n",
       "             ('comedy', 800),\n",
       "             ('sure', 518),\n",
       "             ('humor', 405),\n",
       "             ('astoundingly', 6),\n",
       "             ('flat', 125),\n",
       "             ('occasional', 62),\n",
       "             ('moments', 449),\n",
       "             ('least', 676),\n",
       "             ('bit', 564),\n",
       "             ('touching', 89),\n",
       "             ('charming', 105),\n",
       "             ('entertaining', 313),\n",
       "             ('way', 1668),\n",
       "             ('top', 287),\n",
       "             ('off', 1428),\n",
       "             ('weve', 110),\n",
       "             ('got', 470),\n",
       "             ('birdbrained', 1),\n",
       "             ('spirit', 93),\n",
       "             ('taking', 238),\n",
       "             ('gillians', 1),\n",
       "             ('whatever', 133),\n",
       "             ('handled', 56),\n",
       "             ('terribly', 56),\n",
       "             ('actually', 834),\n",
       "             ('fortune', 46),\n",
       "             ('getting', 404),\n",
       "             ('could', 1395),\n",
       "             ('reason', 435),\n",
       "             ('why', 879),\n",
       "             ('decided', 104),\n",
       "             ('every', 945),\n",
       "             ('standing', 71),\n",
       "             ('sidebyside', 1),\n",
       "             ('attempt', 260),\n",
       "             ('made', 1026),\n",
       "             ('into', 2617),\n",
       "             ('actual', 142),\n",
       "             ('same', 869),\n",
       "             ('goes', 646),\n",
       "             ('aidan', 5),\n",
       "             ('quinn', 32),\n",
       "             ('handsome', 42),\n",
       "             ('police', 239),\n",
       "             ('investigator', 30),\n",
       "             ('misfortune', 3),\n",
       "             ('being', 1336),\n",
       "             ('romantic', 244),\n",
       "             ('opposite', 85),\n",
       "             ('though', 936),\n",
       "             ('minute', 150),\n",
       "             ('mark', 163),\n",
       "             ('once', 601),\n",
       "             ('develop', 69),\n",
       "             ('relationship', 359),\n",
       "             ('dialogue', 516),\n",
       "             ('service', 60),\n",
       "             ('moving', 163),\n",
       "             ('plot', 1451),\n",
       "             ('along', 529),\n",
       "             ('rather', 620),\n",
       "             ('know', 1210),\n",
       "             ('saying', 170),\n",
       "             ('lines', 321),\n",
       "             ('headed', 48),\n",
       "             ('trouble', 156),\n",
       "             ('opening', 301),\n",
       "             ('credits', 201),\n",
       "             ('found', 397),\n",
       "             ('screenplay', 286),\n",
       "             ('written', 365),\n",
       "             ('deeply', 51),\n",
       "             ('hated', 35),\n",
       "             ('akiva', 8),\n",
       "             ('goldsman', 7),\n",
       "             ('managed', 85),\n",
       "             ('destroy', 80),\n",
       "             ('batman', 195),\n",
       "             ('series', 537),\n",
       "             ('forever', 92),\n",
       "             ('robin', 205),\n",
       "             ('directed', 317),\n",
       "             ('griffin', 22),\n",
       "             ('dunne', 21),\n",
       "             ('primarily', 39),\n",
       "             ('last', 837),\n",
       "             ('year', 628),\n",
       "             ('contrived', 77),\n",
       "             ('clumsy', 32),\n",
       "             ('romance', 180),\n",
       "             ('addicted', 13),\n",
       "             ('meg', 56),\n",
       "             ('ryan', 201),\n",
       "             ('matthew', 113),\n",
       "             ('broderick', 51),\n",
       "             ('disliked', 12),\n",
       "             ('quite', 657),\n",
       "             ('worse', 242),\n",
       "             ('give', 561),\n",
       "             ('taste', 71),\n",
       "             ('talentless', 4),\n",
       "             ('filmmaker', 77),\n",
       "             ('id', 197),\n",
       "             ('tell', 341),\n",
       "             ('hes', 1150),\n",
       "             ('ahead', 72),\n",
       "             ('ultimately', 158),\n",
       "             ('already', 285),\n",
       "             ('buried', 25),\n",
       "             ('footloose', 4),\n",
       "             ('goal', 77),\n",
       "             ('reel', 26),\n",
       "             ('cheesy', 71),\n",
       "             ('sentiment', 16),\n",
       "             ('feelgood', 24),\n",
       "             ('antics', 39),\n",
       "             ('admit', 113),\n",
       "             ('fell', 38),\n",
       "             ('teenager', 25),\n",
       "             ('bacon', 57),\n",
       "             ('moves', 141),\n",
       "             ('small', 407),\n",
       "             ('town', 374),\n",
       "             ('city', 437),\n",
       "             ('discovers', 112),\n",
       "             ('dancing', 50),\n",
       "             ('banned', 4),\n",
       "             ('local', 201),\n",
       "             ('reverend', 12),\n",
       "             ('lithgow', 14),\n",
       "             ('ends', 257),\n",
       "             ('falling', 85),\n",
       "             ('preachers', 7),\n",
       "             ('singer', 104),\n",
       "             ('showing', 147),\n",
       "             ('whole', 495),\n",
       "             ('dance', 104),\n",
       "             ('mentioned', 89),\n",
       "             ('beginning', 264),\n",
       "             ('review', 293),\n",
       "             ('extremely', 248),\n",
       "             ('hokey', 38),\n",
       "             ('predictable', 180),\n",
       "             ('drama', 283),\n",
       "             ('virtually', 93),\n",
       "             ('surprises', 71),\n",
       "             ('writing', 187),\n",
       "             ('secondrate', 13),\n",
       "             ('did', 764),\n",
       "             ('enjoy', 227),\n",
       "             ('great', 1141),\n",
       "             ('oozes', 9),\n",
       "             ('charisma', 44),\n",
       "             ('early', 296),\n",
       "             ('role', 863),\n",
       "             ('were', 1458),\n",
       "             ('rooting', 29),\n",
       "             ('unlike', 167),\n",
       "             ('genre', 250),\n",
       "             ('doesnt', 1270),\n",
       "             ('play', 445),\n",
       "             ('rebel', 23),\n",
       "             ('without', 695),\n",
       "             ('sort', 276),\n",
       "             ('yes', 252),\n",
       "             ('does', 1570),\n",
       "             ('polite', 11),\n",
       "             ('needs', 221),\n",
       "             ('broods', 3),\n",
       "             ('equally', 108),\n",
       "             ('impressive', 213),\n",
       "             ('john', 774),\n",
       "             ('playing', 351),\n",
       "             ('normal', 108),\n",
       "             ('normally', 36),\n",
       "             ('villian', 3),\n",
       "             ('concerned', 63),\n",
       "             ('feels', 216),\n",
       "             ('best', 1301),\n",
       "             ('genuine', 99),\n",
       "             ('sense', 553),\n",
       "             ('understanding', 57),\n",
       "             ('lori', 8),\n",
       "             ('nice', 336),\n",
       "             ('touch', 108),\n",
       "             ('see', 1730),\n",
       "             ('kids', 339),\n",
       "             ('complaining', 18),\n",
       "             ('listen', 38),\n",
       "             ('filled', 161),\n",
       "             ('sexual', 183),\n",
       "             ('innuendos', 3),\n",
       "             ('thats', 788),\n",
       "             ('true', 388),\n",
       "             ('title', 290),\n",
       "             ('track', 100),\n",
       "             ('performed', 23),\n",
       "             ('kenny', 19),\n",
       "             ('loggins', 1),\n",
       "             ('upbeat', 17),\n",
       "             ('fun', 571),\n",
       "             ('final', 379),\n",
       "             ('sequence', 364),\n",
       "             ('overcome', 50),\n",
       "             ('urge', 22),\n",
       "             ('couch', 17),\n",
       "             ('course', 647),\n",
       "             ('soon', 382),\n",
       "             ('realized', 78),\n",
       "             ('pathetic', 96),\n",
       "             ('anyway', 190),\n",
       "             ('surprise', 210),\n",
       "             ('dont', 1213),\n",
       "             ('let', 288),\n",
       "             ('premise', 214),\n",
       "             ('prevent', 36),\n",
       "             ('watching', 490),\n",
       "             ('sucked', 34),\n",
       "             ('me', 1401),\n",
       "             ('itll', 26),\n",
       "             ('humorless', 8),\n",
       "             ('officers', 25),\n",
       "             ('changes', 107),\n",
       "             ('befriends', 28),\n",
       "             ('supersmart', 3),\n",
       "             ('superadorable', 1),\n",
       "             ('golden', 48),\n",
       "             ('retriever', 3),\n",
       "             ('named', 353),\n",
       "             ('einstein', 18),\n",
       "             ('cute', 125),\n",
       "             ('young', 738),\n",
       "             ('blond', 29),\n",
       "             ('scientist', 80),\n",
       "             ('unfortunately', 382),\n",
       "             ('shares', 38),\n",
       "             ('psychic', 25),\n",
       "             ('link', 26),\n",
       "             ('bigfootsized', 1),\n",
       "             ('apecreature', 1),\n",
       "             ('trained', 22),\n",
       "             ('unstoppable', 9),\n",
       "             ('killing', 137),\n",
       "             ('machine', 103),\n",
       "             ('rogainenightmare', 1),\n",
       "             ('loose', 56),\n",
       "             ('dog', 232),\n",
       "             ('meanwhile', 133),\n",
       "             ('group', 367),\n",
       "             ('white', 274),\n",
       "             ('chainsmoking', 6),\n",
       "             ('guntoting', 6),\n",
       "             ('nsa', 7),\n",
       "             ('agents', 47),\n",
       "             ('sunglasses', 14),\n",
       "             ('business', 186),\n",
       "             ('suits', 45),\n",
       "             ('kill', 283),\n",
       "             ('comments', 53),\n",
       "             ('watchers', 17),\n",
       "             ('reborn', 16),\n",
       "             ('cheaply', 5),\n",
       "             ('directtovideo', 9),\n",
       "             ('turkey', 40),\n",
       "             ('fourth', 38),\n",
       "             ('sequel', 199),\n",
       "             ('first', 1768),\n",
       "             ('version', 290),\n",
       "             ('dean', 52),\n",
       "             ('koontzs', 4),\n",
       "             ('bestselling', 9),\n",
       "             ('novel', 254),\n",
       "             ('technically', 27),\n",
       "             ('called', 356),\n",
       "             ('v', 25),\n",
       "             ('horror', 433),\n",
       "             ('movies', 1416),\n",
       "             ('sequelcrazy', 1),\n",
       "             ('drop', 49),\n",
       "             ('numbers', 50),\n",
       "             ('titles', 23),\n",
       "             ('star', 678),\n",
       "             ('trek', 154),\n",
       "             ('dropped', 47),\n",
       "             ('vi', 3),\n",
       "             ('makers', 53),\n",
       "             ('want', 557),\n",
       "             ('fool', 42),\n",
       "             ('unsuspecting', 16),\n",
       "             ('rental', 16),\n",
       "             ('customers', 19),\n",
       "             ('thinking', 171),\n",
       "             ('might', 624),\n",
       "             ('instead', 565),\n",
       "             ('crappy', 11),\n",
       "             ('fifth', 52),\n",
       "             ('installment', 45),\n",
       "             ('which', 3155),\n",
       "             ('died', 80),\n",
       "             ('ago', 199),\n",
       "             ('isnt', 870),\n",
       "             ('rented', 14),\n",
       "             ('recieved', 3),\n",
       "             ('sinking', 28),\n",
       "             ('feeling', 225),\n",
       "             ('watched', 80),\n",
       "             ('previews', 31),\n",
       "             ('preceding', 11),\n",
       "             ('feature', 214),\n",
       "             ('presentation', 31),\n",
       "             ('well', 1694),\n",
       "             ('hope', 256),\n",
       "             ('viewer', 193),\n",
       "             ('perhaps', 458),\n",
       "             ('fan', 154),\n",
       "             ('novelist', 8),\n",
       "             ('koontz', 5),\n",
       "             ('hamill', 12),\n",
       "             ('dashed', 3),\n",
       "             ('trailers', 52),\n",
       "             ('tagged', 5),\n",
       "             ('before', 987),\n",
       "             ('theyre', 414),\n",
       "             ('awful', 131),\n",
       "             ('commercials', 34),\n",
       "             ('cover', 95),\n",
       "             ('strange', 184),\n",
       "             ('stripper', 16),\n",
       "             ('shadow', 38),\n",
       "             ('dancer', 29),\n",
       "             ('some', 2982),\n",
       "             ('weird', 97),\n",
       "             ('crap', 90),\n",
       "             ('indian', 33),\n",
       "             ('teens', 44),\n",
       "             ('wolves', 15),\n",
       "             ('action', 1078),\n",
       "             ('detonator', 1),\n",
       "             ('starring', 178),\n",
       "             ('scott', 154),\n",
       "             ('baio', 2),\n",
       "             ('idiot', 26),\n",
       "             ('dreamed', 10),\n",
       "             ('surviving', 26),\n",
       "             ('abysmal', 9),\n",
       "             ('finally', 350),\n",
       "             ('maybe', 345),\n",
       "             ('wont', 262),\n",
       "             ('bad', 1374),\n",
       "             ('bottomofthebarrel', 10),\n",
       "             ('fluff', 26),\n",
       "             ('advertised', 14),\n",
       "             ('misleading', 9),\n",
       "             ('cool', 196),\n",
       "             ('book', 330),\n",
       "             ('wrote', 123),\n",
       "             ('decade', 73),\n",
       "             ('luke', 51),\n",
       "             ('skywalker', 30),\n",
       "             ('terrific', 120),\n",
       "             ('wars', 180),\n",
       "             ('trilogy', 57),\n",
       "             ('lou', 38),\n",
       "             ('rawls', 1),\n",
       "             ('mix', 82),\n",
       "             ('certainly', 360),\n",
       "             ('looks', 444),\n",
       "             ('sick', 75),\n",
       "             ('basic', 123),\n",
       "             ('elements', 236),\n",
       "             ('latter', 104),\n",
       "             ('hack', 24),\n",
       "             ('writer', 169),\n",
       "             ('producing', 26),\n",
       "             ('thrillers', 61),\n",
       "             ('my', 1391),\n",
       "             ('favorite', 155),\n",
       "             ('style', 313),\n",
       "             ('succinct', 2),\n",
       "             ('suspenseful', 60),\n",
       "             ('read', 184),\n",
       "             ...])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the unique words in the data along with the number times they have appeared in the entire dataset\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15ac50",
   "metadata": {},
   "source": [
    "**v. Calculate the average review length and the standard deviation of review\n",
    "lengths. Report the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "f75922e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here a dataframe is created which has the lenght of each of the text present in the combined dataframe\n",
    "review_lenght_df = pd.DataFrame()\n",
    "\n",
    "for index,row in enumerate(combined_dataset['Text']):\n",
    "    row_len = len(row)\n",
    "    review_lenght_df.at[index,\"Review_lenght\"] = row_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "203b9f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2073.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5778.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>4508.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>4050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2293.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3485.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>5576.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Review_lenght\n",
       "0            4188.0\n",
       "1            2073.0\n",
       "2            3187.0\n",
       "3            1841.0\n",
       "4            5778.0\n",
       "...             ...\n",
       "1995         4508.0\n",
       "1996         4050.0\n",
       "1997         2293.0\n",
       "1998         3485.0\n",
       "1999         5576.0\n",
       "\n",
       "[2000 rows x 1 columns]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_lenght_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a8e58b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average lenght of the review text is : 3728.4315\n",
      "The standard deviation of the review text is : 1645.3550792993685\n"
     ]
    }
   ],
   "source": [
    "#Printing the mean and standard deviation of the text lenght \n",
    "print(f\"The average lenght of the review text is : {review_lenght_df['Review_lenght'].mean()}\")\n",
    "print(f\"The standard deviation of the review text is : {review_lenght_df['Review_lenght'].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee28eef",
   "metadata": {},
   "source": [
    "**vi. Plot the histogram of review lengths.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "2efe266d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting a histogram for the review lenghts\n",
    "plt.hist(review_lenght_df, bins= 25)\n",
    "plt.title(\"Histogram for review length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca4241e",
   "metadata": {},
   "source": [
    "**vii. To represent each text (= data point), there are many ways. In NLP/Deep\n",
    "Learning terminology, this task is called tokenization. It is common to represent text using popularity/ rank of words in text. The most common word\n",
    "in the text will be represented as 1, the second most common word will be\n",
    "represented as 2, etc. Tokenize each text document using this method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "25535e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning out of vocab to oov_tok\n",
    "oov_tok = '<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "ce547978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the data\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(combined_dataset['Text'])\n",
    "word_count = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "e0d42718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The priority/rank of words in text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'the': 2,\n",
       " 'a': 3,\n",
       " 'and': 4,\n",
       " 'of': 5,\n",
       " 'to': 6,\n",
       " 'is': 7,\n",
       " 'in': 8,\n",
       " 'that': 9,\n",
       " 'it': 10,\n",
       " 'as': 11,\n",
       " 'with': 12,\n",
       " 'for': 13,\n",
       " 'his': 14,\n",
       " 'this': 15,\n",
       " 'film': 16,\n",
       " 'but': 17,\n",
       " 'he': 18,\n",
       " 'i': 19,\n",
       " 'on': 20,\n",
       " 'are': 21,\n",
       " 'by': 22,\n",
       " 'be': 23,\n",
       " 'its': 24,\n",
       " 'an': 25,\n",
       " 'not': 26,\n",
       " 'one': 27,\n",
       " 'movie': 28,\n",
       " 'who': 29,\n",
       " 'from': 30,\n",
       " 'at': 31,\n",
       " 'was': 32,\n",
       " 'have': 33,\n",
       " 'has': 34,\n",
       " 'her': 35,\n",
       " 'you': 36,\n",
       " 'they': 37,\n",
       " 'all': 38,\n",
       " 'so': 39,\n",
       " 'like': 40,\n",
       " 'about': 41,\n",
       " 'out': 42,\n",
       " 'more': 43,\n",
       " 'when': 44,\n",
       " 'which': 45,\n",
       " 'their': 46,\n",
       " 'up': 47,\n",
       " 'or': 48,\n",
       " 'what': 49,\n",
       " 'some': 50,\n",
       " 'just': 51,\n",
       " 'if': 52,\n",
       " 'there': 53,\n",
       " 'she': 54,\n",
       " 'him': 55,\n",
       " 'into': 56,\n",
       " 'even': 57,\n",
       " 'only': 58,\n",
       " 'than': 59,\n",
       " 'no': 60,\n",
       " 'we': 61,\n",
       " 'good': 62,\n",
       " 'most': 63,\n",
       " 'time': 64,\n",
       " 'can': 65,\n",
       " 'will': 66,\n",
       " 'story': 67,\n",
       " 'films': 68,\n",
       " 'been': 69,\n",
       " 'would': 70,\n",
       " 'much': 71,\n",
       " 'also': 72,\n",
       " 'characters': 73,\n",
       " 'other': 74,\n",
       " 'get': 75,\n",
       " 'character': 76,\n",
       " 'do': 77,\n",
       " 'them': 78,\n",
       " 'very': 79,\n",
       " 'two': 80,\n",
       " 'first': 81,\n",
       " 'after': 82,\n",
       " 'see': 83,\n",
       " 'well': 84,\n",
       " 'because': 85,\n",
       " 'way': 86,\n",
       " 'make': 87,\n",
       " 'any': 88,\n",
       " 'does': 89,\n",
       " 'really': 90,\n",
       " 'had': 91,\n",
       " 'too': 92,\n",
       " 'while': 93,\n",
       " 'how': 94,\n",
       " 'little': 95,\n",
       " 'life': 96,\n",
       " 'where': 97,\n",
       " 'were': 98,\n",
       " 'plot': 99,\n",
       " 'off': 100,\n",
       " 'people': 101,\n",
       " 'movies': 102,\n",
       " 'then': 103,\n",
       " 'me': 104,\n",
       " 'could': 105,\n",
       " 'my': 106,\n",
       " 'bad': 107,\n",
       " 'scene': 108,\n",
       " 'never': 109,\n",
       " 'being': 110,\n",
       " 'these': 111,\n",
       " 'over': 112,\n",
       " 'best': 113,\n",
       " 'new': 114,\n",
       " 'man': 115,\n",
       " 'doesnt': 116,\n",
       " 'many': 117,\n",
       " 'scenes': 118,\n",
       " 'such': 119,\n",
       " 'dont': 120,\n",
       " 'know': 121,\n",
       " 'through': 122,\n",
       " 'hes': 123,\n",
       " 'great': 124,\n",
       " 'another': 125,\n",
       " 'here': 126,\n",
       " 's': 127,\n",
       " 'love': 128,\n",
       " 'action': 129,\n",
       " 'go': 130,\n",
       " 'us': 131,\n",
       " 'director': 132,\n",
       " 'something': 133,\n",
       " 'end': 134,\n",
       " 'still': 135,\n",
       " 'back': 136,\n",
       " 'seems': 137,\n",
       " 'made': 138,\n",
       " 'those': 139,\n",
       " 'work': 140,\n",
       " 'theres': 141,\n",
       " 'makes': 142,\n",
       " 'before': 143,\n",
       " 'however': 144,\n",
       " 'now': 145,\n",
       " 'big': 146,\n",
       " 'years': 147,\n",
       " 'few': 148,\n",
       " 'world': 149,\n",
       " 'between': 150,\n",
       " 'every': 151,\n",
       " 'though': 152,\n",
       " 'seen': 153,\n",
       " 'better': 154,\n",
       " 'enough': 155,\n",
       " 'around': 156,\n",
       " 'take': 157,\n",
       " 'both': 158,\n",
       " 'performance': 159,\n",
       " 'why': 160,\n",
       " 'audience': 161,\n",
       " 'down': 162,\n",
       " 'going': 163,\n",
       " 'isnt': 164,\n",
       " 'same': 165,\n",
       " 'gets': 166,\n",
       " 'should': 167,\n",
       " 'role': 168,\n",
       " 'real': 169,\n",
       " 'may': 170,\n",
       " 'things': 171,\n",
       " 'your': 172,\n",
       " 'think': 173,\n",
       " 'last': 174,\n",
       " 'actually': 175,\n",
       " 'look': 176,\n",
       " 'funny': 177,\n",
       " 'own': 178,\n",
       " 'almost': 179,\n",
       " 'thing': 180,\n",
       " 'say': 181,\n",
       " 'nothing': 182,\n",
       " 'comedy': 183,\n",
       " 'fact': 184,\n",
       " 'although': 185,\n",
       " 'thats': 186,\n",
       " 'played': 187,\n",
       " 'right': 188,\n",
       " 'find': 189,\n",
       " 'john': 190,\n",
       " 'come': 191,\n",
       " 'since': 192,\n",
       " 'did': 193,\n",
       " 'script': 194,\n",
       " 'cast': 195,\n",
       " 'plays': 196,\n",
       " 'long': 197,\n",
       " 'young': 198,\n",
       " 'ever': 199,\n",
       " 'comes': 200,\n",
       " 'old': 201,\n",
       " 'actors': 202,\n",
       " 'original': 203,\n",
       " 'part': 204,\n",
       " 'show': 205,\n",
       " 'without': 206,\n",
       " 'acting': 207,\n",
       " 'each': 208,\n",
       " 'again': 209,\n",
       " 'star': 210,\n",
       " 'least': 211,\n",
       " 'lot': 212,\n",
       " 'point': 213,\n",
       " 'takes': 214,\n",
       " 'quite': 215,\n",
       " 'himself': 216,\n",
       " 'during': 217,\n",
       " 'away': 218,\n",
       " 'course': 219,\n",
       " 'goes': 220,\n",
       " 'cant': 221,\n",
       " 'minutes': 222,\n",
       " 'interesting': 223,\n",
       " 'effects': 224,\n",
       " 'year': 225,\n",
       " 'three': 226,\n",
       " 'im': 227,\n",
       " 'screen': 228,\n",
       " 'might': 229,\n",
       " 'family': 230,\n",
       " 'guy': 231,\n",
       " 'rather': 232,\n",
       " 'anything': 233,\n",
       " 'day': 234,\n",
       " 'far': 235,\n",
       " 'place': 236,\n",
       " 'must': 237,\n",
       " 'watch': 238,\n",
       " 'once': 239,\n",
       " 'our': 240,\n",
       " 'yet': 241,\n",
       " 'didnt': 242,\n",
       " 'always': 243,\n",
       " 'seem': 244,\n",
       " 'fun': 245,\n",
       " 'times': 246,\n",
       " 'trying': 247,\n",
       " 'instead': 248,\n",
       " 'bit': 249,\n",
       " 'special': 250,\n",
       " 'making': 251,\n",
       " 'give': 252,\n",
       " 'want': 253,\n",
       " 'sense': 254,\n",
       " 'job': 255,\n",
       " 'picture': 256,\n",
       " 'kind': 257,\n",
       " 'wife': 258,\n",
       " 'having': 259,\n",
       " 'home': 260,\n",
       " 'set': 261,\n",
       " 'probably': 262,\n",
       " 'series': 263,\n",
       " 'help': 264,\n",
       " 'along': 265,\n",
       " 'becomes': 266,\n",
       " 'pretty': 267,\n",
       " 'hollywood': 268,\n",
       " 'everything': 269,\n",
       " 'sure': 270,\n",
       " 'american': 271,\n",
       " 'men': 272,\n",
       " 'dialogue': 273,\n",
       " 'woman': 274,\n",
       " 'together': 275,\n",
       " 'actor': 276,\n",
       " 'become': 277,\n",
       " 'gives': 278,\n",
       " 'hard': 279,\n",
       " 'money': 280,\n",
       " 'given': 281,\n",
       " 'high': 282,\n",
       " 'whole': 283,\n",
       " 'black': 284,\n",
       " 'watching': 285,\n",
       " 'music': 286,\n",
       " 'wants': 287,\n",
       " 'got': 288,\n",
       " 'feel': 289,\n",
       " 'done': 290,\n",
       " 'perhaps': 291,\n",
       " 'especially': 292,\n",
       " 'death': 293,\n",
       " 'less': 294,\n",
       " 'next': 295,\n",
       " 'moments': 296,\n",
       " 'sex': 297,\n",
       " 'everyone': 298,\n",
       " 'play': 299,\n",
       " 'looks': 300,\n",
       " 'completely': 301,\n",
       " 'city': 302,\n",
       " 'looking': 303,\n",
       " 'reason': 304,\n",
       " 'whose': 305,\n",
       " 'horror': 306,\n",
       " 'shows': 307,\n",
       " 'rest': 308,\n",
       " 'until': 309,\n",
       " 'performances': 310,\n",
       " 'different': 311,\n",
       " 'simply': 312,\n",
       " 'james': 313,\n",
       " 'father': 314,\n",
       " 'ending': 315,\n",
       " 'friends': 316,\n",
       " 'couple': 317,\n",
       " 'put': 318,\n",
       " 'case': 319,\n",
       " 'several': 320,\n",
       " 'mind': 321,\n",
       " 'theyre': 322,\n",
       " 'night': 323,\n",
       " 'evil': 324,\n",
       " 'left': 325,\n",
       " 'anyone': 326,\n",
       " 'michael': 327,\n",
       " 'human': 328,\n",
       " 'shes': 329,\n",
       " 'small': 330,\n",
       " 'itself': 331,\n",
       " 'entire': 332,\n",
       " 'humor': 333,\n",
       " 'girl': 334,\n",
       " 'getting': 335,\n",
       " 'lost': 336,\n",
       " 'turns': 337,\n",
       " 'line': 338,\n",
       " 'main': 339,\n",
       " 'found': 340,\n",
       " 'use': 341,\n",
       " 'problem': 342,\n",
       " 'half': 343,\n",
       " 'begins': 344,\n",
       " 'true': 345,\n",
       " 'either': 346,\n",
       " 'stars': 347,\n",
       " 'ive': 348,\n",
       " 'soon': 349,\n",
       " 'unfortunately': 350,\n",
       " 'mother': 351,\n",
       " 'later': 352,\n",
       " 'final': 353,\n",
       " 'idea': 354,\n",
       " 'name': 355,\n",
       " 'someone': 356,\n",
       " 'school': 357,\n",
       " 'comic': 358,\n",
       " 'town': 359,\n",
       " 'thought': 360,\n",
       " 'wrong': 361,\n",
       " 'else': 362,\n",
       " 'based': 363,\n",
       " 'friend': 364,\n",
       " 'tries': 365,\n",
       " 'alien': 366,\n",
       " 'house': 367,\n",
       " 'group': 368,\n",
       " 'against': 369,\n",
       " 'second': 370,\n",
       " 'david': 371,\n",
       " 'dead': 372,\n",
       " 'written': 373,\n",
       " 'sequence': 374,\n",
       " 'keep': 375,\n",
       " 'used': 376,\n",
       " 'often': 377,\n",
       " 'certainly': 378,\n",
       " 'relationship': 379,\n",
       " 'works': 380,\n",
       " 'believe': 381,\n",
       " 'called': 382,\n",
       " 'said': 383,\n",
       " 'named': 384,\n",
       " 'despite': 385,\n",
       " 'playing': 386,\n",
       " 'behind': 387,\n",
       " 'head': 388,\n",
       " 'finally': 389,\n",
       " 'turn': 390,\n",
       " 'war': 391,\n",
       " 'under': 392,\n",
       " 'maybe': 393,\n",
       " 'days': 394,\n",
       " 'doing': 395,\n",
       " 'tell': 396,\n",
       " 'able': 397,\n",
       " 'kids': 398,\n",
       " 'finds': 399,\n",
       " 'seeing': 400,\n",
       " 'nice': 401,\n",
       " 'perfect': 402,\n",
       " 'youre': 403,\n",
       " 'past': 404,\n",
       " 'hand': 405,\n",
       " 'book': 406,\n",
       " 'including': 407,\n",
       " 'mr': 408,\n",
       " 'person': 409,\n",
       " 'shot': 410,\n",
       " 'live': 411,\n",
       " 'lives': 412,\n",
       " 'boy': 413,\n",
       " 'run': 414,\n",
       " 'camera': 415,\n",
       " 'supposed': 416,\n",
       " 'lines': 417,\n",
       " 'moment': 418,\n",
       " 'tv': 419,\n",
       " 'directed': 420,\n",
       " 'side': 421,\n",
       " 'starts': 422,\n",
       " 'need': 423,\n",
       " 'fight': 424,\n",
       " 'summer': 425,\n",
       " 'entertaining': 426,\n",
       " 'style': 427,\n",
       " 'game': 428,\n",
       " 'running': 429,\n",
       " 'car': 430,\n",
       " 'full': 431,\n",
       " 'worth': 432,\n",
       " 'dark': 433,\n",
       " 'face': 434,\n",
       " 'worst': 435,\n",
       " 'start': 436,\n",
       " 'kevin': 437,\n",
       " 'try': 438,\n",
       " 'upon': 439,\n",
       " 'matter': 440,\n",
       " 'others': 441,\n",
       " 'nearly': 442,\n",
       " 'care': 443,\n",
       " 'hour': 444,\n",
       " 'son': 445,\n",
       " 'opening': 446,\n",
       " 'throughout': 447,\n",
       " 'example': 448,\n",
       " 'violence': 449,\n",
       " 'exactly': 450,\n",
       " 'video': 451,\n",
       " 'daughter': 452,\n",
       " 'early': 453,\n",
       " 'major': 454,\n",
       " 'beautiful': 455,\n",
       " 'problems': 456,\n",
       " 'review': 457,\n",
       " 'sequences': 458,\n",
       " 'short': 459,\n",
       " 'title': 460,\n",
       " 'version': 461,\n",
       " 'production': 462,\n",
       " 'wasnt': 463,\n",
       " 'let': 464,\n",
       " 'robert': 465,\n",
       " 'whos': 466,\n",
       " 'top': 467,\n",
       " 'joe': 468,\n",
       " 'obvious': 469,\n",
       " 'screenplay': 470,\n",
       " 'classic': 471,\n",
       " 'team': 472,\n",
       " 'already': 473,\n",
       " 'guys': 474,\n",
       " 'drama': 475,\n",
       " 'kill': 476,\n",
       " 'fine': 477,\n",
       " 'direction': 478,\n",
       " 'children': 479,\n",
       " 'eyes': 480,\n",
       " 'order': 481,\n",
       " 'simple': 482,\n",
       " 'roles': 483,\n",
       " 'themselves': 484,\n",
       " 'hit': 485,\n",
       " 'knows': 486,\n",
       " 'question': 487,\n",
       " 'act': 488,\n",
       " 'sort': 489,\n",
       " 'supporting': 490,\n",
       " 'earth': 491,\n",
       " 'white': 492,\n",
       " 'truly': 493,\n",
       " 'deep': 494,\n",
       " 'save': 495,\n",
       " 'sometimes': 496,\n",
       " 'boring': 497,\n",
       " 'jack': 498,\n",
       " 'known': 499,\n",
       " 'women': 500,\n",
       " 'beginning': 501,\n",
       " 'scream': 502,\n",
       " 'wont': 503,\n",
       " 'coming': 504,\n",
       " 'hell': 505,\n",
       " 'attempt': 506,\n",
       " 'killer': 507,\n",
       " 'four': 508,\n",
       " 'jokes': 509,\n",
       " 'tom': 510,\n",
       " 'strong': 511,\n",
       " 'arent': 512,\n",
       " 'space': 513,\n",
       " 'body': 514,\n",
       " 'happens': 515,\n",
       " 'ends': 516,\n",
       " 'room': 517,\n",
       " 'york': 518,\n",
       " 'hope': 519,\n",
       " 'says': 520,\n",
       " 'heart': 521,\n",
       " 'jackie': 522,\n",
       " 'tells': 523,\n",
       " 'novel': 524,\n",
       " 'possible': 525,\n",
       " 'peter': 526,\n",
       " 'yes': 527,\n",
       " 'saw': 528,\n",
       " 'quickly': 529,\n",
       " 'stupid': 530,\n",
       " 'genre': 531,\n",
       " 'five': 532,\n",
       " 'lead': 533,\n",
       " 'extremely': 534,\n",
       " 'manages': 535,\n",
       " 'girls': 536,\n",
       " 'wonder': 537,\n",
       " 'murder': 538,\n",
       " 'lee': 539,\n",
       " 'particularly': 540,\n",
       " 'romantic': 541,\n",
       " 'level': 542,\n",
       " 'stop': 543,\n",
       " 'ship': 544,\n",
       " 'future': 545,\n",
       " 'appears': 546,\n",
       " 'involving': 547,\n",
       " 'worse': 548,\n",
       " 'career': 549,\n",
       " 'involved': 550,\n",
       " 'voice': 551,\n",
       " 'mostly': 552,\n",
       " 'thriller': 553,\n",
       " 'eventually': 554,\n",
       " 'sets': 555,\n",
       " 'falls': 556,\n",
       " 'police': 557,\n",
       " 'hours': 558,\n",
       " 'sound': 559,\n",
       " 'taking': 560,\n",
       " 'attention': 561,\n",
       " 'emotional': 562,\n",
       " 'dr': 563,\n",
       " 'result': 564,\n",
       " 'material': 565,\n",
       " 'elements': 566,\n",
       " 'hero': 567,\n",
       " 'ones': 568,\n",
       " 'planet': 569,\n",
       " 'bring': 570,\n",
       " 'lack': 571,\n",
       " 'close': 572,\n",
       " 'whats': 573,\n",
       " 'piece': 574,\n",
       " 'meet': 575,\n",
       " 'child': 576,\n",
       " 'note': 577,\n",
       " 'experience': 578,\n",
       " 'none': 579,\n",
       " 'fall': 580,\n",
       " 'dog': 581,\n",
       " 'van': 582,\n",
       " 'brother': 583,\n",
       " 'fans': 584,\n",
       " 'leads': 585,\n",
       " 'fiction': 586,\n",
       " 'living': 587,\n",
       " 'alone': 588,\n",
       " 'wild': 589,\n",
       " 'de': 590,\n",
       " 'enjoy': 591,\n",
       " 'battle': 592,\n",
       " 'theater': 593,\n",
       " 'obviously': 594,\n",
       " 'interest': 595,\n",
       " 'guess': 596,\n",
       " 'youll': 597,\n",
       " 'paul': 598,\n",
       " 'feeling': 599,\n",
       " 'among': 600,\n",
       " 'taken': 601,\n",
       " 'usually': 602,\n",
       " 'late': 603,\n",
       " 'husband': 604,\n",
       " 'laugh': 605,\n",
       " 'laughs': 606,\n",
       " 'parents': 607,\n",
       " 'king': 608,\n",
       " 'george': 609,\n",
       " 'power': 610,\n",
       " 'mean': 611,\n",
       " 'aliens': 612,\n",
       " 'needs': 613,\n",
       " 'happen': 614,\n",
       " 'attempts': 615,\n",
       " 'within': 616,\n",
       " 'talent': 617,\n",
       " 'chance': 618,\n",
       " 'number': 619,\n",
       " 'across': 620,\n",
       " 'single': 621,\n",
       " 'deal': 622,\n",
       " 'chris': 623,\n",
       " 'brothers': 624,\n",
       " 'williams': 625,\n",
       " 'talk': 626,\n",
       " 'forced': 627,\n",
       " 'success': 628,\n",
       " 'feels': 629,\n",
       " 'wonderful': 630,\n",
       " 'whether': 631,\n",
       " 'features': 632,\n",
       " 'god': 633,\n",
       " 'easy': 634,\n",
       " 'history': 635,\n",
       " 'killed': 636,\n",
       " 'expect': 637,\n",
       " 'premise': 638,\n",
       " 'feature': 639,\n",
       " 'leave': 640,\n",
       " 'television': 641,\n",
       " 'words': 642,\n",
       " 'word': 643,\n",
       " 'impressive': 644,\n",
       " 'poor': 645,\n",
       " 'science': 646,\n",
       " 'mission': 647,\n",
       " 'except': 648,\n",
       " 'giving': 649,\n",
       " 'form': 650,\n",
       " 'tale': 651,\n",
       " 'call': 652,\n",
       " 'recent': 653,\n",
       " 'seemed': 654,\n",
       " 'score': 655,\n",
       " 'meets': 656,\n",
       " 'surprise': 657,\n",
       " 'basically': 658,\n",
       " 'disney': 659,\n",
       " 'oscar': 660,\n",
       " 'apparently': 661,\n",
       " 'serious': 662,\n",
       " 'filmmakers': 663,\n",
       " 'told': 664,\n",
       " 'important': 665,\n",
       " 'crew': 666,\n",
       " 'entertainment': 667,\n",
       " 'released': 668,\n",
       " 'parts': 669,\n",
       " 'easily': 670,\n",
       " 'somehow': 671,\n",
       " 'stuff': 672,\n",
       " 'robin': 673,\n",
       " 'computer': 674,\n",
       " 'change': 675,\n",
       " 'happy': 676,\n",
       " 'brings': 677,\n",
       " 'events': 678,\n",
       " 'credits': 679,\n",
       " 'ryan': 680,\n",
       " 'local': 681,\n",
       " 'whom': 682,\n",
       " 'hilarious': 683,\n",
       " 'art': 684,\n",
       " 'am': 685,\n",
       " 'difficult': 686,\n",
       " 'remember': 687,\n",
       " 'release': 688,\n",
       " 'went': 689,\n",
       " 'working': 690,\n",
       " 'crime': 691,\n",
       " 'sequel': 692,\n",
       " 'ago': 693,\n",
       " 'oh': 694,\n",
       " 'wouldnt': 695,\n",
       " 'certain': 696,\n",
       " 'using': 697,\n",
       " 'lets': 698,\n",
       " 'middle': 699,\n",
       " 'id': 700,\n",
       " 'complete': 701,\n",
       " 'william': 702,\n",
       " 'cool': 703,\n",
       " 'audiences': 704,\n",
       " 'girlfriend': 705,\n",
       " 'batman': 706,\n",
       " 'runs': 707,\n",
       " 'due': 708,\n",
       " 'return': 709,\n",
       " 'turned': 710,\n",
       " 'effective': 711,\n",
       " 'ben': 712,\n",
       " 'viewer': 713,\n",
       " 'suspense': 714,\n",
       " 'reality': 715,\n",
       " 'smith': 716,\n",
       " 'ill': 717,\n",
       " 'flick': 718,\n",
       " 'presence': 719,\n",
       " 'quality': 720,\n",
       " 'popular': 721,\n",
       " 'uses': 722,\n",
       " 'anyway': 723,\n",
       " 'dramatic': 724,\n",
       " 'mystery': 725,\n",
       " 'personal': 726,\n",
       " 'surprisingly': 727,\n",
       " 'begin': 728,\n",
       " 'decides': 729,\n",
       " 'die': 730,\n",
       " 'figure': 731,\n",
       " 'youve': 732,\n",
       " 'ways': 733,\n",
       " 'writing': 734,\n",
       " 'somewhat': 735,\n",
       " 'viewers': 736,\n",
       " 'annoying': 737,\n",
       " 'absolutely': 738,\n",
       " 'similar': 739,\n",
       " 'business': 740,\n",
       " 'previous': 741,\n",
       " 'shots': 742,\n",
       " 'blood': 743,\n",
       " 'couldnt': 744,\n",
       " 'came': 745,\n",
       " 'light': 746,\n",
       " 'excellent': 747,\n",
       " 'gone': 748,\n",
       " 'strange': 749,\n",
       " 'read': 750,\n",
       " 'sexual': 751,\n",
       " 'means': 752,\n",
       " 'latest': 753,\n",
       " 'project': 754,\n",
       " 'former': 755,\n",
       " 'nor': 756,\n",
       " 'rich': 757,\n",
       " 'towards': 758,\n",
       " 'successful': 759,\n",
       " 'leaves': 760,\n",
       " 'intelligent': 761,\n",
       " 'amazing': 762,\n",
       " 'familiar': 763,\n",
       " 'visual': 764,\n",
       " 'romance': 765,\n",
       " 'predictable': 766,\n",
       " 'wars': 767,\n",
       " 'beyond': 768,\n",
       " 'leaving': 769,\n",
       " 'following': 770,\n",
       " 'present': 771,\n",
       " 'jim': 772,\n",
       " 'questions': 773,\n",
       " 'cut': 774,\n",
       " 'myself': 775,\n",
       " 'clear': 776,\n",
       " 'starring': 777,\n",
       " 'type': 778,\n",
       " 'kid': 779,\n",
       " 'message': 780,\n",
       " 'definitely': 781,\n",
       " 'talking': 782,\n",
       " 'herself': 783,\n",
       " 'powerful': 784,\n",
       " 'add': 785,\n",
       " 'brilliant': 786,\n",
       " 'party': 787,\n",
       " 'nature': 788,\n",
       " 'situation': 789,\n",
       " 'opens': 790,\n",
       " 'clever': 791,\n",
       " 'secret': 792,\n",
       " 'red': 793,\n",
       " 'create': 794,\n",
       " 'giant': 795,\n",
       " 'felt': 796,\n",
       " 'villain': 797,\n",
       " 'office': 798,\n",
       " 'stories': 799,\n",
       " 'usual': 800,\n",
       " 'straight': 801,\n",
       " 'smart': 802,\n",
       " 'third': 803,\n",
       " 'scary': 804,\n",
       " 'cinema': 805,\n",
       " 'cop': 806,\n",
       " 'company': 807,\n",
       " 'bunch': 808,\n",
       " 'actress': 809,\n",
       " 'learn': 810,\n",
       " 'large': 811,\n",
       " 'rock': 812,\n",
       " 'age': 813,\n",
       " 'doubt': 814,\n",
       " 'prison': 815,\n",
       " 'bill': 816,\n",
       " 'move': 817,\n",
       " 'thinking': 818,\n",
       " 'solid': 819,\n",
       " 'water': 820,\n",
       " 'jones': 821,\n",
       " 'saying': 822,\n",
       " 'follows': 823,\n",
       " 'bob': 824,\n",
       " 'million': 825,\n",
       " 'writer': 826,\n",
       " 'huge': 827,\n",
       " 'effect': 828,\n",
       " 'potential': 829,\n",
       " 'seriously': 830,\n",
       " 'america': 831,\n",
       " 'near': 832,\n",
       " 'plan': 833,\n",
       " 'unlike': 834,\n",
       " 'general': 835,\n",
       " 'animated': 836,\n",
       " 'realize': 837,\n",
       " 'perfectly': 838,\n",
       " 'follow': 839,\n",
       " 'decent': 840,\n",
       " 'took': 841,\n",
       " 'understand': 842,\n",
       " 'likely': 843,\n",
       " 'motion': 844,\n",
       " 'martin': 845,\n",
       " 'sam': 846,\n",
       " 'mark': 847,\n",
       " 'moving': 848,\n",
       " 'subject': 849,\n",
       " 'enjoyable': 850,\n",
       " 'happened': 851,\n",
       " 'immediately': 852,\n",
       " 'married': 853,\n",
       " 'created': 854,\n",
       " 'agent': 855,\n",
       " 'heard': 856,\n",
       " 'force': 857,\n",
       " 'filled': 858,\n",
       " 'stay': 859,\n",
       " 'th': 860,\n",
       " 'above': 861,\n",
       " 'fails': 862,\n",
       " 'sweet': 863,\n",
       " 'points': 864,\n",
       " 'exciting': 865,\n",
       " 'merely': 866,\n",
       " 'country': 867,\n",
       " 'slow': 868,\n",
       " 'wedding': 869,\n",
       " 'overall': 870,\n",
       " 'break': 871,\n",
       " 'wanted': 872,\n",
       " 'neither': 873,\n",
       " 'ultimately': 874,\n",
       " 'escape': 875,\n",
       " 'impossible': 876,\n",
       " 'dream': 877,\n",
       " 'appear': 878,\n",
       " 'bruce': 879,\n",
       " 'brought': 880,\n",
       " 'mess': 881,\n",
       " 'directors': 882,\n",
       " 'richard': 883,\n",
       " 'private': 884,\n",
       " 'trouble': 885,\n",
       " 'inside': 886,\n",
       " 'favorite': 887,\n",
       " 'r': 888,\n",
       " 'tim': 889,\n",
       " 'musical': 890,\n",
       " 'murphy': 891,\n",
       " 'liked': 892,\n",
       " 'trek': 893,\n",
       " 'fan': 894,\n",
       " 'scott': 895,\n",
       " 'pay': 896,\n",
       " 'particular': 897,\n",
       " 'various': 898,\n",
       " 'political': 899,\n",
       " 'otherwise': 900,\n",
       " 'keeps': 901,\n",
       " 'ten': 902,\n",
       " 'dumb': 903,\n",
       " 'steve': 904,\n",
       " 'chase': 905,\n",
       " 'situations': 906,\n",
       " 'talented': 907,\n",
       " 'minute': 908,\n",
       " 'members': 909,\n",
       " 'spend': 910,\n",
       " 'harry': 911,\n",
       " 'society': 912,\n",
       " 'element': 913,\n",
       " 'studio': 914,\n",
       " 'bond': 915,\n",
       " 'truth': 916,\n",
       " 'effort': 917,\n",
       " 'silly': 918,\n",
       " 'earlier': 919,\n",
       " 'biggest': 920,\n",
       " 'focus': 921,\n",
       " 'slightly': 922,\n",
       " 'rating': 923,\n",
       " 'drug': 924,\n",
       " 'offers': 925,\n",
       " 'showing': 926,\n",
       " 'purpose': 927,\n",
       " 'open': 928,\n",
       " 'havent': 929,\n",
       " 'cannot': 930,\n",
       " 'soundtrack': 931,\n",
       " 'park': 932,\n",
       " 'eye': 933,\n",
       " 'memorable': 934,\n",
       " 'fast': 935,\n",
       " 'frank': 936,\n",
       " 'totally': 937,\n",
       " 'english': 938,\n",
       " 'cold': 939,\n",
       " 'mars': 940,\n",
       " 'state': 941,\n",
       " 'ideas': 942,\n",
       " 'view': 943,\n",
       " 'gun': 944,\n",
       " 'waste': 945,\n",
       " 'aspect': 946,\n",
       " 'ask': 947,\n",
       " 'eddie': 948,\n",
       " 'subplot': 949,\n",
       " 'credit': 950,\n",
       " 'box': 951,\n",
       " 'wait': 952,\n",
       " 'government': 953,\n",
       " 'entirely': 954,\n",
       " 'constantly': 955,\n",
       " 'hands': 956,\n",
       " 'actual': 957,\n",
       " 'fear': 958,\n",
       " 'law': 959,\n",
       " 'l': 960,\n",
       " 'moves': 961,\n",
       " 'gave': 962,\n",
       " 'terrible': 963,\n",
       " 'e': 964,\n",
       " 'british': 965,\n",
       " 'west': 966,\n",
       " 'ability': 967,\n",
       " 'convincing': 968,\n",
       " 'u': 969,\n",
       " 'spent': 970,\n",
       " 'female': 971,\n",
       " 'ridiculous': 972,\n",
       " 'typical': 973,\n",
       " 'setting': 974,\n",
       " 'thinks': 975,\n",
       " 'atmosphere': 976,\n",
       " 'air': 977,\n",
       " 'cinematography': 978,\n",
       " 'animation': 979,\n",
       " 'fairly': 980,\n",
       " 'lots': 981,\n",
       " 'carter': 982,\n",
       " 'killing': 983,\n",
       " 'suddenly': 984,\n",
       " 'expected': 985,\n",
       " 'control': 986,\n",
       " 'depth': 987,\n",
       " 'background': 988,\n",
       " 'beauty': 989,\n",
       " 'greatest': 990,\n",
       " 'tension': 991,\n",
       " 'sees': 992,\n",
       " 'humans': 993,\n",
       " 'critics': 994,\n",
       " 'army': 995,\n",
       " 'sit': 996,\n",
       " 'seven': 997,\n",
       " 'amusing': 998,\n",
       " 'brief': 999,\n",
       " 'violent': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#printing the ranking of the words that have appeared the most in order\n",
    "print(f\"The priority/rank of words in text\")\n",
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c041f2a",
   "metadata": {},
   "source": [
    "**viii. Select a review length L that 70% of the reviews have a length below it. If\n",
    "you feel more adventurous, set the threshold to 90%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "4537d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the review lenght for which 70% of the texts have lenght less that lenght\n",
    "lenght_L = review_lenght_df['Review_lenght'].quantile(0.7,interpolation='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "9a6720ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4267.0"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lenght value\n",
    "lenght_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "76cc8103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>4267.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Review_lenght\n",
       "1079         4267.0"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the from the dataframe\n",
    "review_lenght_df[review_lenght_df['Review_lenght'] == lenght_L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "a65e9e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_lenght</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4188.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2073.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3187.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1841.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>2375.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>1340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>4050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2293.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3485.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Review_lenght\n",
       "0            4188.0\n",
       "1            2073.0\n",
       "2            3187.0\n",
       "3            1841.0\n",
       "5            2205.0\n",
       "...             ...\n",
       "1992         2375.0\n",
       "1994         1340.0\n",
       "1996         4050.0\n",
       "1997         2293.0\n",
       "1998         3485.0\n",
       "\n",
       "[1400 rows x 1 columns]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying all the text/reviews lower than the selected lenght\n",
    "#there are a total of 2000 reviews, out of which 70% data is 1400, which can be seen below\n",
    "review_lenght_df.loc[review_lenght_df['Review_lenght'].sort_values() <= lenght_L]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ecb81",
   "metadata": {},
   "source": [
    "**ix. Truncate reviews longer than L words and zero-pad reviews shorter than L\n",
    "so that all texts (= data points) are of length L.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "f1746b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5128, 2502,    4, ...,    0,    0,    0],\n",
       "       [1968, 3740,  699, ...,    0,    0,    0],\n",
       "       [5526, 1444,    7, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1968,   80, 2619, ...,    0,    0,    0],\n",
       "       [ 433,  302,    7, ...,    0,    0,    0],\n",
       "       [  44,  101,   21, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Truncating and zero padding text/review depending on the length obtained\n",
    "combined_df_sequence = tokenizer.texts_to_sequences(combined_dataset['Text'])\n",
    "\n",
    "padded_data = keras.preprocessing.sequence.pad_sequences(combined_df_sequence,\n",
    "                                                         maxlen = int(lenght_L),\n",
    "                                                         padding ='post',\n",
    "                                                         truncating ='post')\n",
    "padded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c81c5",
   "metadata": {},
   "source": [
    "## (c) Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67390b82",
   "metadata": {},
   "source": [
    "**i. One can use tokenized text as inputs to a deep neural network. However, a re￾cent breakthrough in NLP suggests that more sophisticated representations of\n",
    "text yield better results. These sophisticated representations are called word\n",
    "embeddings. “Word embedding is a term used for representation of words\n",
    "for text analysis, typically in the form of a real-valued vector that encodes\n",
    "the meaning of the word such that the words that are closer in the vector\n",
    "space are expected to be similar in meaning.”4\n",
    ". Most deep learning modules\n",
    "(including Keras) provide a convenient way to convert positive integer rep￾resentations of words into a word embedding by an “Embedding layer.” The\n",
    "layer accepts arguments that define the mapping of words into embeddings,including the maximum number of expected words also called the vocabulary\n",
    "size (e.g. the largest integer value). The layer also allows you to specify the\n",
    "dimension for each word vector, called the “output dimension.” We would like\n",
    "to use a word embedding layer for this project. Assume that we are inter￾ested in the top 5,000 words. This means that in each integer sequence that\n",
    "represents each document, we set to zero those integers that represent words\n",
    "that are not among the top 5,000 words in the document.5\n",
    "If you feel more\n",
    "adventurous, use all the words that appear in this corpus. Choose the length\n",
    "of the embedding vector for each word to be 32. Hence, each document is\n",
    "represented as a 32 × L matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54034068",
   "metadata": {},
   "source": [
    "**ii. Flatten the matrix of each document to a vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "43666e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make the labels into a numpy array\n",
    "sentiments = combined_dataset['Labels'].to_numpy()\n",
    "sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "b3557057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the data by setting number of words to 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = 5000)\n",
    "tokenizer.fit_on_texts(combined_dataset['Text'])\n",
    "word_count_new = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "5e36fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4267.0\n"
     ]
    }
   ],
   "source": [
    "#finding the lenght L for which 70% of the data has lower lenght \n",
    "lenght_L = review_lenght_df['Review_lenght'].quantile(0.7,interpolation='lower')\n",
    "print(lenght_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "5cbbf92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2501,    3,   15, ...,    0,    0,    0],\n",
       "       [1967, 3739,  698, ...,    0,    0,    0],\n",
       "       [1443,    6,    2, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1967,   79, 2618, ...,    0,    0,    0],\n",
       "       [ 432,  301,    6, ...,    0,    0,    0],\n",
       "       [  43,  100,   20, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trucanting and zero padding text\n",
    "combined_df_sequence = tokenizer.texts_to_sequences(combined_dataset['Text'])\n",
    "\n",
    "padded_data_new = keras.preprocessing.sequence.pad_sequences(combined_df_sequence,\n",
    "                                                         maxlen = int(lenght_L),\n",
    "                                                         padding ='post',\n",
    "                                                         truncating ='post')\n",
    "padded_data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "58aa13e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "63/63 [==============================] - 6s 84ms/step - loss: 4.4901 - accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "63/63 [==============================] - 6s 94ms/step - loss: 4.3247 - accuracy: 0.0000e+00\n",
      "(2000, 136544)\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_66 (Embedding)    (None, 4267, 32)          160032    \n",
      "                                                                 \n",
      " flatten_52 (Flatten)        (None, 136544)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,032\n",
      "Trainable params: 160,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Word Embedding on whole dataset\n",
    "vocab_size = 5000\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Embedding\n",
    "embedding = Embedding(input_dim=vocab_size+1, output_dim=32,input_length=int(lenght_L))\n",
    "model.add(embedding)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#compiling the model using the adam optimizer and binary crossentropy loss\n",
    "model.compile(optimizer= 'adam',loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x = padded_data_new,y = sentiments, epochs=2 , verbose=1)\n",
    "output_data = model.predict(padded_data_new)\n",
    "\n",
    "print(output_data.shape)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216312c",
   "metadata": {},
   "source": [
    "## (d) Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27def4db",
   "metadata": {},
   "source": [
    "**i. Train a MLP with three (dense) hidden layers each of which has 50 ReLUs\n",
    "and one output layer with a single sigmoid neuron. Use a dropout rate of\n",
    "20% for the first layer and 50% for the other layers. Use ADAM optimizer\n",
    "and binary cross entropy loss (which is equivalent to having a softmax in the\n",
    "output). To avoid overfitting, just set the number of epochs as 2. Use a batch\n",
    "size of 10.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "acf1334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trucanting and zero padding  training data\n",
    "train_df_sequence = tokenizer.texts_to_sequences(df_train['Text'])\n",
    "padded_data_train = keras.preprocessing.sequence.pad_sequences(train_df_sequence,\n",
    "                                                         maxlen = int(lenght_L),\n",
    "                                                         padding ='post',\n",
    "                                                         truncating ='post')\n",
    "sentiments_train = df_train['Labels'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "130a59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trucanting and zero padding testing data\n",
    "test_df_sequence = tokenizer.texts_to_sequences(df_test['Text'])\n",
    "padded_data_test = keras.preprocessing.sequence.pad_sequences(test_df_sequence,\n",
    "                                                         maxlen = int(lenght_L),\n",
    "                                                         padding ='post',\n",
    "                                                         truncating ='post')\n",
    "sentiments_test = df_test['Labels'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "43a6caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "112/112 [==============================] - 8s 69ms/step - loss: 0.7770 - accuracy: 0.4929 - val_loss: 0.6937 - val_accuracy: 0.4714\n",
      "Epoch 2/2\n",
      "112/112 [==============================] - 7s 62ms/step - loss: 0.6934 - accuracy: 0.5071 - val_loss: 0.6935 - val_accuracy: 0.4714\n",
      "(1400, 1)\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_67 (Embedding)    (None, 4267, 32)          160032    \n",
      "                                                                 \n",
      " flatten_53 (Flatten)        (None, 136544)            0         \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 50)                6827250   \n",
      "                                                                 \n",
      " dropout_65 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_66 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_67 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,992,433\n",
      "Trainable params: 6,992,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#MLP implementation\n",
    "vocab_size = 5000\n",
    "model = Sequential()\n",
    "embedding = Embedding(input_dim=vocab_size+1, output_dim=32,input_length=int(lenght_L))\n",
    "model.add(embedding)\n",
    "model.add(Flatten())\n",
    "\n",
    "#Dense layers\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer= 'adam',loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x = padded_data_train,y = sentiments_train, epochs=2 , verbose=1, batch_size = 10, validation_split=0.2)\n",
    "output_data_train = model.predict(padded_data_train)\n",
    "output_data_test = model.predict(padded_data_test)\n",
    "\n",
    "print(output_data_train.shape)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc6aa4",
   "metadata": {},
   "source": [
    "**ii. Report the train and test accuracies of this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "e8effa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 - 3s - loss: 0.6932 - accuracy: 0.5007 - 3s/epoch - 69ms/step\n",
      "Train Accuracy: 50.07143020629883\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_data_train, sentiments_train, verbose=2)\n",
    "print(f\"Train Accuracy: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "1e13b1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 - 1s - loss: 0.6932 - accuracy: 0.5000 - 1s/epoch - 72ms/step\n",
      "Test Accuracy: 50.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_data_test, sentiments_test, verbose=2)\n",
    "print(f\"Test Accuracy: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f10e7",
   "metadata": {},
   "source": [
    "## (e)One-Dimensional Convolutional Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b200544",
   "metadata": {},
   "source": [
    "**Although CNNs are mainly used for image data, they can also be applied to text\n",
    "data, as text also has adjacency information. Keras supports one-dimensional\n",
    "convolutions and pooling by the Conv1D and MaxPooling1D classes respectively.**\n",
    "\n",
    "**i. After the embedding layer, insert a Conv1D layer. This convolutional layer\n",
    "has 32 feature maps , and each of the 32 kernels has size 3, i.e. reads embedded\n",
    "word representations 3 vector elements of the word embedding at a time. The\n",
    "convolutional layer is followed by a 1D max pooling layer with a length and\n",
    "stride of 2 that halves the size of the feature maps from the convolutional\n",
    "layer. The rest of the network is the same as the neural network above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "989a17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "140/140 [==============================] - 8s 55ms/step - loss: 0.8097 - accuracy: 0.4814\n",
      "Epoch 2/2\n",
      "140/140 [==============================] - 8s 56ms/step - loss: 0.6939 - accuracy: 0.5007\n",
      "(1400, 1)\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_68 (Embedding)    (None, 4267, 32)          160032    \n",
      "                                                                 \n",
      " flatten_54 (Flatten)        (None, 136544)            0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 50)                6827250   \n",
      "                                                                 \n",
      " dropout_68 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_69 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_70 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,992,433\n",
      "Trainable params: 6,992,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "vocab_size = 5000\n",
    "model_cnn = Sequential()\n",
    "embedding = Embedding(input_dim=vocab_size+1, output_dim=32,input_length=int(lenght_L))\n",
    "model_cnn.add(embedding)\n",
    "\n",
    "#CNN and maxpooling\n",
    "Conv1D(filters = 32, kernel_size=3)\n",
    "MaxPooling1D(pool_size=2, strides=2)\n",
    "\n",
    "model_cnn.add(Flatten())\n",
    "\n",
    "#Desnce layers\n",
    "model_cnn.add(Dense(50, activation='relu'))\n",
    "model_cnn.add(Dropout(0.2))\n",
    "\n",
    "model_cnn.add(Dense(50, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "\n",
    "model_cnn.add(Dense(50, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "\n",
    "model_cnn.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model_cnn.compile(optimizer= 'adam',loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_cnn.fit(x = padded_data_train,y = sentiments_train, epochs=2 , verbose=1, batch_size = 10)\n",
    "output_data_train = model_cnn.predict(padded_data_train)\n",
    "output_data_test = model_cnn.predict(padded_data_test)\n",
    "\n",
    "print(output_data_train.shape)\n",
    "print(model_cnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7001601",
   "metadata": {},
   "source": [
    "**ii. Report the train and test accuracies of this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "2e42d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 50.07143020629883\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_cnn.evaluate(padded_data_train, sentiments_train, verbose=0)\n",
    "print(f\"Train Accuracy: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "5bb9a454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 - 1s - loss: 0.6932 - accuracy: 0.5000 - 1s/epoch - 65ms/step\n",
      "Test Accuracy: 50.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_cnn.evaluate(padded_data_test, sentiments_test, verbose=2)\n",
    "print(f\"Test Accuracy: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5710a",
   "metadata": {},
   "source": [
    "## (f) Long Short-Term Memory Recurrent Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e9db99",
   "metadata": {},
   "source": [
    "**The structure of the LSTM we are going to use is shown in the following figure.**\n",
    "\n",
    "**i. Each word is represented to LSTM as a vector of 32 elements and the LSTM\n",
    "is followed by a dense layer of 256 ReLUs. Use a dropout rate of 0.2 for both\n",
    "LSTM and the dense layer. Train the model using 10-50 epochs and batch\n",
    "size of 10.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "c908f3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "140/140 [==============================] - 514s 4s/step - loss: 0.6944 - accuracy: 0.4886\n",
      "Epoch 2/30\n",
      "140/140 [==============================] - 428s 3s/step - loss: 0.6936 - accuracy: 0.4807\n",
      "Epoch 3/30\n",
      "140/140 [==============================] - 498s 4s/step - loss: 0.6935 - accuracy: 0.5036\n",
      "Epoch 4/30\n",
      "140/140 [==============================] - 486s 3s/step - loss: 0.6937 - accuracy: 0.4793\n",
      "Epoch 5/30\n",
      "140/140 [==============================] - 485s 3s/step - loss: 0.6934 - accuracy: 0.4893\n",
      "Epoch 6/30\n",
      "140/140 [==============================] - 490s 4s/step - loss: 0.6934 - accuracy: 0.4943\n",
      "Epoch 7/30\n",
      "140/140 [==============================] - 983s 7s/step - loss: 0.6937 - accuracy: 0.4629\n",
      "Epoch 8/30\n",
      "140/140 [==============================] - 398s 3s/step - loss: 0.6935 - accuracy: 0.4714\n",
      "Epoch 9/30\n",
      "140/140 [==============================] - 485s 3s/step - loss: 0.6935 - accuracy: 0.4843\n",
      "Epoch 10/30\n",
      "140/140 [==============================] - 493s 4s/step - loss: 0.6932 - accuracy: 0.5007\n",
      "Epoch 11/30\n",
      "140/140 [==============================] - 484s 3s/step - loss: 0.6933 - accuracy: 0.4914\n",
      "Epoch 12/30\n",
      "140/140 [==============================] - 490s 4s/step - loss: 0.6934 - accuracy: 0.4950\n",
      "Epoch 13/30\n",
      "140/140 [==============================] - 487s 3s/step - loss: 0.6934 - accuracy: 0.4900\n",
      "Epoch 14/30\n",
      "140/140 [==============================] - 3307s 24s/step - loss: 0.6935 - accuracy: 0.4921\n",
      "Epoch 15/30\n",
      "140/140 [==============================] - 459s 3s/step - loss: 0.6935 - accuracy: 0.4921\n",
      "Epoch 16/30\n",
      "140/140 [==============================] - 489s 3s/step - loss: 0.6934 - accuracy: 0.4807\n",
      "Epoch 17/30\n",
      "140/140 [==============================] - 486s 3s/step - loss: 0.6934 - accuracy: 0.4993\n",
      "Epoch 18/30\n",
      "140/140 [==============================] - 483s 3s/step - loss: 0.6934 - accuracy: 0.4807\n",
      "Epoch 19/30\n",
      "140/140 [==============================] - 486s 3s/step - loss: 0.6931 - accuracy: 0.5029\n",
      "Epoch 20/30\n",
      "140/140 [==============================] - 487s 3s/step - loss: 0.6932 - accuracy: 0.5007\n",
      "Epoch 21/30\n",
      "140/140 [==============================] - 494s 4s/step - loss: 0.6937 - accuracy: 0.4821\n",
      "Epoch 22/30\n",
      "140/140 [==============================] - 489s 3s/step - loss: 0.6934 - accuracy: 0.4643\n",
      "Epoch 23/30\n",
      "140/140 [==============================] - 489s 3s/step - loss: 0.6934 - accuracy: 0.5036\n",
      "Epoch 24/30\n",
      "140/140 [==============================] - 487s 3s/step - loss: 0.6935 - accuracy: 0.4850\n",
      "Epoch 25/30\n",
      "140/140 [==============================] - 485s 3s/step - loss: 0.6934 - accuracy: 0.4843\n",
      "Epoch 26/30\n",
      "140/140 [==============================] - 486s 3s/step - loss: 0.6934 - accuracy: 0.4814\n",
      "Epoch 27/30\n",
      "140/140 [==============================] - 486s 3s/step - loss: 0.6933 - accuracy: 0.4829\n",
      "Epoch 28/30\n",
      "140/140 [==============================] - 490s 3s/step - loss: 0.6934 - accuracy: 0.4821\n",
      "Epoch 29/30\n",
      "140/140 [==============================] - 7647s 55s/step - loss: 0.6932 - accuracy: 0.4843\n",
      "Epoch 30/30\n",
      "140/140 [==============================] - 491s 4s/step - loss: 0.6934 - accuracy: 0.4764\n",
      "(1400, 1)\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_69 (Embedding)    (None, 4267, 32)          160032    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " flatten_55 (Flatten)        (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_71 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 256)               8448      \n",
      "                                                                 \n",
      " dropout_72 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 177,057\n",
      "Trainable params: 177,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#LSTM implementation \n",
    "vocab_size = 5000\n",
    "model_LSTM = Sequential()\n",
    "embedding = Embedding(input_dim=vocab_size+1, output_dim=32,input_length=int(lenght_L))\n",
    "model_LSTM.add(embedding)\n",
    "\n",
    "#LSTM with 32 elements\n",
    "model_LSTM.add(LSTM(32))\n",
    "model_LSTM.add(Flatten())\n",
    "model_LSTM.add(Dropout(0.2))\n",
    "\n",
    "#Dense layer with 256 Relus\n",
    "model_LSTM.add(Dense(256, activation='relu'))\n",
    "model_LSTM.add(Dropout(0.2))\n",
    "\n",
    "model_LSTM.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model_LSTM.compile(optimizer= 'adam',loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#number of epochs - 30\n",
    "model_LSTM.fit(x = padded_data_train,y = sentiments_train, epochs=30 , verbose=1, batch_size = 10)\n",
    "output_data_train = model_LSTM.predict(padded_data_train)\n",
    "output_data_test = model_LSTM.predict(padded_data_test)\n",
    "\n",
    "print(output_data_train.shape)\n",
    "print(model_LSTM.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c1a45",
   "metadata": {},
   "source": [
    "**ii. Report the train and test accuracies of this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "59f28bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 50.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_LSTM.evaluate(padded_data_train, sentiments_train, verbose=0)\n",
    "print(f\"Train Accuracy: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "820cc476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 - 12s - loss: 0.6931 - accuracy: 0.5000 - 12s/epoch - 635ms/step\n",
      "Test Accuracy: 50.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_LSTM.evaluate(padded_data_test, sentiments_test, verbose=2)\n",
    "print(f\"Test Accuracy: {accuracy*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d7ad8",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d7c6d",
   "metadata": {},
   "source": [
    "1) https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer <br>\n",
    "2) https://www.tensorflow.org/api_docs/python/tf/keras<br>\n",
    "3) https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory<br>\n",
    "4) https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/<br>\n",
    "5) https://www.tensorflow.org/api_docs/python/tf/keras/Sequential<br>\n",
    "6) https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding<br>\n",
    "7) https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten<br>\n",
    "8) https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense<br>\n",
    "9) https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout<br>\n",
    "10) https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D<br>\n",
    "11) https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D<br>\n",
    "12) https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd6334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
